# -*- coding: utf-8 -*-
"""Emotion Classification Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jRUFRQmGodo6Pm3r1WkDj7VLyCQyMCnb
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
df = pd.read_csv('drive/MyDrive/EmotionTrainingDataset/Emotion_final.csv' ,nrows=4000)

df

cat = pd.get_dummies(df.Emotion)
df = pd.concat([df, cat], axis=1)
df = df.drop(columns='Emotion')

df

emotion_text = df['Text'].values
label = df[['anger','fear','happy','love','sadness','surprise']].values

emotion_text

label

from sklearn.model_selection import train_test_split

text = df['Text'].values
y = df[['anger','fear','happy','love','sadness','surprise']].values
text_train , text_test, y_train, y_test = train_test_split(text, y, test_size=0.2)

import tensorflow as tf
reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(
    monitor='val_loss', 
    factor=0.2,
    patience=5, 
    min_lr=1.5e-5
)

early_stop = tf.keras.callbacks.EarlyStopping(
    monitor="val_loss",
    min_delta=0,
    patience=12,
    verbose=0,
    mode="auto",
    baseline=None,
    restore_best_weights=True
)

import matplotlib.pyplot as plt

plt.style.use('seaborn-whitegrid')

def plot_acc(history):
  acc = history.history['accuracy']
  val_acc = history.history['val_accuracy']
  epochs = range(len(acc))
  plt.subplot(1, 2, 1)
  acc_plot, = plt.plot(epochs, acc, 'r')
  val_acc_plot, = plt.plot(epochs, val_acc, 'b')
  plt.title('Training and Validation Accuracy')
  plt.legend([acc_plot, val_acc_plot], ['Training Accuracy', 'Validation Accuracy'])


def plot_loss(history):
  loss = history.history['loss']
  val_loss = history.history['val_loss']
  epochs = range(len(loss))
  plt.subplot(1, 2, 2)
  loss_plot, = plt.plot(epochs, loss, 'r')
  val_loss_plot, = plt.plot(epochs, val_loss, 'b')
  plt.title('Training and Validation Loss')
  plt.legend([loss_plot, val_loss_plot], ['Training Loss', 'Validation Loss'])

def plot_history(history):
  plt.figure(figsize=(15,5))
  plot_acc(history)
  plot_loss(history)

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

#tokenizer
tokenizer = Tokenizer(num_words=20000)
tokenizer.fit_on_texts(text_train) 
tokenizer.fit_on_texts(text_test)
 
#sequence 
seq_train = tokenizer.texts_to_sequences(text_train)
seq_test = tokenizer.texts_to_sequences(text_test)
 
#padding
pad_train = pad_sequences(seq_train)
pad_test = pad_sequences(seq_test)

tf.keras.backend.clear_session()

model = tf.keras.models.Sequential([
    tf.keras.layers.Embedding(input_dim=20000, output_dim=64),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.LSTM(64),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Dense(6, activation='softmax')
])
model.summary()

model.compile(
    optimizer='adam', 
    loss='binary_crossentropy', 
    metrics=['accuracy']
)

history_emot = model.fit(
    pad_train, 
    y_train,
    epochs=100,
    callbacks = [reduce_lr, early_stop],
    verbose=1,
    validation_data=(pad_test, y_test)
)

plot_history(history_emot)